{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output matrix shape: (3, 12)\n",
      "[[4.34668214e+01 3.94642356e+01 5.56716441e+01 5.66825338e+01\n",
      "  3.56639409e+01 3.73573684e+01 3.93370176e+01 4.95249003e+01\n",
      "  3.57495586e+01 5.15701875e+01 4.58689552e+01 4.10114466e+01]\n",
      " [3.26192644e-16 3.02689267e-16 4.26371550e-16 4.34286530e-16\n",
      "  2.76677853e-16 2.84378781e-16 3.08742926e-16 3.88121678e-16\n",
      "  2.80779464e-16 4.03171632e-16 3.56820676e-16 3.20965314e-16]\n",
      " [5.32273872e-07 4.93918127e-07 6.95735281e-07 7.08652029e-07\n",
      "  4.51468684e-07 4.64039867e-07 5.03788292e-07 6.33314413e-07\n",
      "  4.58159905e-07 6.57869505e-07 5.82244275e-07 5.23731237e-07]]\n",
      "Refactored output matrix shape: (3, 12)\n",
      "[[4.34668214e+01 3.94642356e+01 5.56716441e+01 5.66825338e+01\n",
      "  3.56639409e+01 3.73573684e+01 3.93370176e+01 4.95249003e+01\n",
      "  3.57495586e+01 5.15701875e+01 4.58689552e+01 4.10114466e+01]\n",
      " [3.26192644e-16 3.02689267e-16 4.26371550e-16 4.34286530e-16\n",
      "  2.76677853e-16 2.84378781e-16 3.08742926e-16 3.88121678e-16\n",
      "  2.80779464e-16 4.03171632e-16 3.56820676e-16 3.20965314e-16]\n",
      " [5.32273872e-07 4.93918127e-07 6.95735281e-07 7.08652029e-07\n",
      "  4.51468684e-07 4.64039867e-07 5.03788292e-07 6.33314413e-07\n",
      "  4.58159905e-07 6.57869505e-07 5.82244275e-07 5.23731237e-07]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def single_head_attention_with_weights(x, W_O, W_V, W_K, W_Q):\n",
    "    \"\"\"\n",
    "    Implements single-head attention mechanism with weight matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (n, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (n, d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute Query, Key, Value matrices from input and corresponding weight matrices (n x d_model)\n",
    "    Q = np.dot(x, W_Q)\n",
    "    K = np.dot(x, W_K)\n",
    "    V = np.dot(x, W_V)\n",
    "    \n",
    "    # Compute attention scores (n x n)\n",
    "    attention_scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Apply softmax to get attention distribution\n",
    "    attention_weights = softmax(attention_scores)\n",
    "    \n",
    "    # Compute weighted sum of value vectors (n x d_model)\n",
    "    weighted_sum = np.dot(attention_weights, V)\n",
    "    \n",
    "    # Apply the output weight matrix W_O (n x d_model)\n",
    "    output = np.dot(weighted_sum, W_O)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def single_head_attention_refactored(x, W_O, W_V, W_K, W_Q):\n",
    "    \"\"\"\n",
    "    Implements single-head attention mechanism with weight matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Input matrix of shape (n, d_model)\n",
    "    - W_O, W_V, W_K, W_Q: Weight matrices, each of shape (d_model, d_model)\n",
    "    \n",
    "    Returns:\n",
    "    - Output matrix after attention and linear transformations, of shape (n, d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute Query, Key, Value matrices from input and corresponding weight matrices (n x d_model)\n",
    "    Q = np.dot(x, W_Q)\n",
    "    K = np.dot(x, W_K)\n",
    "    \n",
    "    # Compute attention scores (n x n)\n",
    "    attention_scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Apply softmax to get attention distribution\n",
    "    A = softmax(attention_scores)\n",
    "\n",
    "    # Get W_V W_O transformation matrix\n",
    "    W_V_O = np.dot(W_V, W_O)\n",
    "    \n",
    "    # Compute output matrix\n",
    "    result = np.dot(A, np.dot(x, W_V_O))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "n = 3  # Number of tokens\n",
    "d_model = 12  # Dimensionality of each token\n",
    "\n",
    "# Randomly initialize input matrix and weight matrices\n",
    "x = np.random.rand(n, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "\n",
    "# Compute the output after single-head attention and linear transformations\n",
    "output = single_head_attention_with_weights(x, W_O, W_V, W_K, W_Q)\n",
    "print(\"Output matrix shape:\", output.shape)\n",
    "print(output)\n",
    "\n",
    "refactored_output = single_head_attention_refactored(x, W_O, W_V, W_K, W_Q)\n",
    "print(\"Refactored output matrix shape:\", refactored_output.shape)\n",
    "print(refactored_output)\n",
    "\n",
    "assert np.allclose(output, refactored_output), \"The refactored function produces different results!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Steps\n",
      "Refactored Function\n",
      "Q_ref: [[2.74164853 2.16543522 3.11395906 1.81372275 3.61045065 2.25350768\n",
      "  2.4131675  1.75332325 2.64397586 2.36416384 3.40024893 2.80104433]\n",
      " [3.86351318 3.06026361 3.73121894 2.31102605 4.31653228 2.19899644\n",
      "  3.5773023  2.39155238 3.38761674 3.42601296 4.26252272 3.38465289]\n",
      " [3.12868281 2.78401292 2.6377517  2.07847662 3.97412076 2.39345565\n",
      "  2.89580702 2.36366808 2.98321191 3.41700392 3.74121058 2.30881082]]\n",
      "K_ref: [[2.55193091 2.28271    3.0672082  3.19981717 1.79893658 2.38405286\n",
      "  3.20266548 3.22662727 3.01204698 2.64811881 2.62990482 2.91258509]\n",
      " [3.18274375 3.12399677 3.44239717 3.91213621 2.6854836  3.70796548\n",
      "  3.30034205 4.54121761 4.24141233 3.5039551  2.9552325  4.00057431]\n",
      " [2.89827177 3.22737594 2.62614944 3.39441195 2.47542763 2.55954154\n",
      "  2.85613994 3.43214411 3.47064866 3.59597897 2.54266209 3.22358044]]\n",
      "attention_scores_ref: [[ 83.87261953 108.03645489  92.23718861]\n",
      " [108.20991025 138.66504892 119.28251361]\n",
      " [ 93.43469087 120.62366395 103.97866888]]\n",
      "A_ref: [[2.69431750e-11 4.99080521e-14 1.79624054e-12]\n",
      " [9.99999617e-01 9.99999985e-01 9.99999774e-01]\n",
      " [3.83004248e-07 1.46125515e-08 2.25748350e-07]]\n",
      "W_V_O_ref: [[3.62226636 3.4209508  2.73776753 3.11510157 2.59672586 2.74416176\n",
      "  2.18398788 1.8643374  3.03008206 2.91147465 2.39414312 1.71828949]\n",
      " [3.61092468 3.66060588 2.61020155 3.09988501 2.68245831 2.66754545\n",
      "  2.33075634 2.02860018 3.03945385 3.68371055 2.73043208 1.3438243 ]\n",
      " [2.73282621 3.20520002 1.76646029 2.02764182 2.61024386 1.82579492\n",
      "  1.44419717 1.3584937  2.13860445 1.83780262 2.0073367  1.19838786]\n",
      " [3.25823193 3.19342288 2.25513934 3.33019222 2.81108584 2.70650645\n",
      "  2.36208211 1.77728263 3.29830182 2.81312198 2.0907074  1.68676403]\n",
      " [4.66944638 4.54801846 3.85844786 4.46182321 3.16838432 3.55825793\n",
      "  2.98549112 2.50345402 4.1129854  3.71699085 3.56088752 2.48990176]\n",
      " [3.41702426 3.24058185 2.66456466 3.01645145 2.52214399 2.75859798\n",
      "  2.03371317 1.72694207 3.48770199 3.15653256 2.85255341 1.87881864]\n",
      " [2.79713514 4.03073785 1.95884117 2.00782495 2.90377337 1.79603077\n",
      "  1.86339934 1.4424017  2.21672743 2.46012154 2.35502217 0.7786    ]\n",
      " [3.49917637 3.37097323 2.98350491 3.09120459 2.44142686 2.77578947\n",
      "  2.25057643 1.71123039 3.30602068 2.7806415  2.71705667 2.01895781]\n",
      " [3.19514395 3.37798018 1.94424292 2.6337607  2.75522175 2.2280203\n",
      "  2.28717244 1.86768662 2.38053345 2.50938531 2.01295394 1.05663934]\n",
      " [3.5794867  3.73490204 2.98498161 3.06870639 2.31852404 2.36312744\n",
      "  2.27875402 1.63325004 3.193582   2.62597721 3.25212281 1.52588515]\n",
      " [3.02041468 3.33612395 2.85592262 2.37728803 2.24957857 2.61134676\n",
      "  2.08828413 1.50491662 2.55063059 2.44852585 2.45602812 1.78888568]\n",
      " [2.42791971 2.53355922 1.65709524 1.50512801 1.0273191  1.54034591\n",
      "  1.37223194 0.78761416 1.98156724 1.46968711 2.05228684 0.67776216]]\n",
      "result_ref: [[5.22110333e-10 5.41716364e-10 4.14374530e-10 4.45117453e-10\n",
      "  3.85596002e-10 3.94790098e-10 3.35960678e-10 2.61574631e-10\n",
      "  4.63244169e-10 4.27290035e-10 4.08919328e-10 2.49825575e-10]\n",
      " [6.13048205e+01 6.40919810e+01 4.75511729e+01 5.24374991e+01\n",
      "  4.60795502e+01 4.59495474e+01 3.91915044e+01 3.09557014e+01\n",
      "  5.40271184e+01 4.98300626e+01 4.73723197e+01 2.88810067e+01]\n",
      " [1.17635302e-05 1.22090770e-05 9.25745839e-06 1.00341474e-05\n",
      "  8.65714163e-06 8.82993872e-06 7.52829751e-06 5.88495104e-06\n",
      "  1.03959401e-05 9.51630775e-06 9.19905191e-06 5.59154730e-06]]\n",
      "\n",
      "Einsum Function\n",
      "Q_ein: [[2.74164853 2.16543522 3.11395906 1.81372275 3.61045065 2.25350768\n",
      "  2.4131675  1.75332325 2.64397586 2.36416384 3.40024893 2.80104433]\n",
      " [3.86351318 3.06026361 3.73121894 2.31102605 4.31653228 2.19899644\n",
      "  3.5773023  2.39155238 3.38761674 3.42601296 4.26252272 3.38465289]\n",
      " [3.12868281 2.78401292 2.6377517  2.07847662 3.97412076 2.39345565\n",
      "  2.89580702 2.36366808 2.98321191 3.41700392 3.74121058 2.30881082]]\n",
      "K_ein: [[2.55193091 2.28271    3.0672082  3.19981717 1.79893658 2.38405286\n",
      "  3.20266548 3.22662727 3.01204698 2.64811881 2.62990482 2.91258509]\n",
      " [3.18274375 3.12399677 3.44239717 3.91213621 2.6854836  3.70796548\n",
      "  3.30034205 4.54121761 4.24141233 3.5039551  2.9552325  4.00057431]\n",
      " [2.89827177 3.22737594 2.62614944 3.39441195 2.47542763 2.55954154\n",
      "  2.85613994 3.43214411 3.47064866 3.59597897 2.54266209 3.22358044]]\n",
      "attention_scores_ein: [[ 83.87261953 108.03645489  92.23718861]\n",
      " [108.20991025 138.66504892 119.28251361]\n",
      " [ 93.43469087 120.62366395 103.97866888]]\n",
      "A_ein: [[2.69431750e-11 4.99080521e-14 1.79624054e-12]\n",
      " [9.99999617e-01 9.99999985e-01 9.99999774e-01]\n",
      " [3.83004248e-07 1.46125515e-08 2.25748350e-07]]\n",
      "W_V_O_ein: [[3.62226636 3.4209508  2.73776753 3.11510157 2.59672586 2.74416176\n",
      "  2.18398788 1.8643374  3.03008206 2.91147465 2.39414312 1.71828949]\n",
      " [3.61092468 3.66060588 2.61020155 3.09988501 2.68245831 2.66754545\n",
      "  2.33075634 2.02860018 3.03945385 3.68371055 2.73043208 1.3438243 ]\n",
      " [2.73282621 3.20520002 1.76646029 2.02764182 2.61024386 1.82579492\n",
      "  1.44419717 1.3584937  2.13860445 1.83780262 2.0073367  1.19838786]\n",
      " [3.25823193 3.19342288 2.25513934 3.33019222 2.81108584 2.70650645\n",
      "  2.36208211 1.77728263 3.29830182 2.81312198 2.0907074  1.68676403]\n",
      " [4.66944638 4.54801846 3.85844786 4.46182321 3.16838432 3.55825793\n",
      "  2.98549112 2.50345402 4.1129854  3.71699085 3.56088752 2.48990176]\n",
      " [3.41702426 3.24058185 2.66456466 3.01645145 2.52214399 2.75859798\n",
      "  2.03371317 1.72694207 3.48770199 3.15653256 2.85255341 1.87881864]\n",
      " [2.79713514 4.03073785 1.95884117 2.00782495 2.90377337 1.79603077\n",
      "  1.86339934 1.4424017  2.21672743 2.46012154 2.35502217 0.7786    ]\n",
      " [3.49917637 3.37097323 2.98350491 3.09120459 2.44142686 2.77578947\n",
      "  2.25057643 1.71123039 3.30602068 2.7806415  2.71705667 2.01895781]\n",
      " [3.19514395 3.37798018 1.94424292 2.6337607  2.75522175 2.2280203\n",
      "  2.28717244 1.86768662 2.38053345 2.50938531 2.01295394 1.05663934]\n",
      " [3.5794867  3.73490204 2.98498161 3.06870639 2.31852404 2.36312744\n",
      "  2.27875402 1.63325004 3.193582   2.62597721 3.25212281 1.52588515]\n",
      " [3.02041468 3.33612395 2.85592262 2.37728803 2.24957857 2.61134676\n",
      "  2.08828413 1.50491662 2.55063059 2.44852585 2.45602812 1.78888568]\n",
      " [2.42791971 2.53355922 1.65709524 1.50512801 1.0273191  1.54034591\n",
      "  1.37223194 0.78761416 1.98156724 1.46968711 2.05228684 0.67776216]]\n",
      "result_ein: [[5.18123247e-10 5.37583078e-10 4.11901549e-10 4.41679980e-10\n",
      "  3.83026029e-10 3.92389336e-10 3.33784029e-10 2.59682934e-10\n",
      "  4.60087607e-10 4.25104736e-10 4.05883447e-10 2.48250528e-10]\n",
      " [6.97097947e+01 7.38295909e+01 5.29574511e+01 5.99062937e+01\n",
      "  5.45257331e+01 5.23801977e+01 4.46492149e+01 3.58306953e+01\n",
      "  6.12932115e+01 5.76140650e+01 5.27465630e+01 3.24574766e+01]\n",
      " [1.25116660e-05 1.29716822e-05 9.71904427e-06 1.06763392e-05\n",
      "  9.10104628e-06 9.26308416e-06 7.92572381e-06 6.22868158e-06\n",
      "  1.09804933e-05 9.88615034e-06 9.78168133e-06 5.88384676e-06]]\n",
      "\n",
      "Comparison Results\n",
      "Q matrices close: True\n",
      "K matrices close: True\n",
      "Attention scores close: True\n",
      "A matrices close: True\n",
      "W_V_O matrices close: True\n",
      "Final result matrices close: False\n"
     ]
    }
   ],
   "source": [
    "# Function to debug and compare each step in the refactored and einsum functions\n",
    "def debug_attention_steps(x, W_O, W_V, W_K, W_Q):\n",
    "    print(\"Debugging Steps\")\n",
    "    \n",
    "    # ---- Refactored Function ----\n",
    "    print(\"Refactored Function\")\n",
    "    Q_ref = np.dot(x, W_Q)\n",
    "    K_ref = np.dot(x, W_K)\n",
    "    attention_scores_ref = np.dot(Q_ref, K_ref.T)\n",
    "    A_ref = softmax(attention_scores_ref)\n",
    "    W_V_O_ref = np.dot(W_V, W_O)\n",
    "    result_ref = np.dot(A_ref, np.dot(x, W_V_O_ref))\n",
    "    print(\"Q_ref:\", Q_ref)\n",
    "    print(\"K_ref:\", K_ref)\n",
    "    print(\"attention_scores_ref:\", attention_scores_ref)\n",
    "    print(\"A_ref:\", A_ref)\n",
    "    print(\"W_V_O_ref:\", W_V_O_ref)\n",
    "    print(\"result_ref:\", result_ref)\n",
    "    \n",
    "    # ---- Einsum Function ----\n",
    "    print(\"\\nEinsum Function\")\n",
    "    Q_ein = np.einsum('nd,df->nf', x, W_Q)\n",
    "    K_ein = np.einsum('nd,df->nf', x, W_K)\n",
    "    attention_scores_ein = np.einsum('nf,mf->nm', Q_ein, K_ein)\n",
    "    A_ein = softmax(attention_scores_ein)\n",
    "    W_V_O_ein = np.einsum('df,fg->dg', W_V, W_O)\n",
    "    result_ein = np.einsum('nm,nd,dg->ng', A_ein, x, W_V_O_ein)\n",
    "    print(\"Q_ein:\", Q_ein)\n",
    "    print(\"K_ein:\", K_ein)\n",
    "    print(\"attention_scores_ein:\", attention_scores_ein)\n",
    "    print(\"A_ein:\", A_ein)\n",
    "    print(\"W_V_O_ein:\", W_V_O_ein)\n",
    "    print(\"result_ein:\", result_ein)\n",
    "    \n",
    "    # Compare each step\n",
    "    print(\"\\nComparison Results\")\n",
    "    print(\"Q matrices close:\", np.allclose(Q_ref, Q_ein))\n",
    "    print(\"K matrices close:\", np.allclose(K_ref, K_ein))\n",
    "    print(\"Attention scores close:\", np.allclose(attention_scores_ref, attention_scores_ein))\n",
    "    print(\"A matrices close:\", np.allclose(A_ref, A_ein))\n",
    "    print(\"W_V_O matrices close:\", np.allclose(W_V_O_ref, W_V_O_ein))\n",
    "    print(\"Final result matrices close:\", np.allclose(result_ref, result_ein))\n",
    "\n",
    "# Debugging with a new set of random weights and input\n",
    "x = np.random.rand(n, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "\n",
    "# Run the debugging function\n",
    "debug_attention_steps(x, W_O, W_V, W_K, W_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Final Result\n",
      "result_ref: [[3.68681062e+01 4.15191810e+01 5.20266937e+01 2.58056177e+01\n",
      "  5.47430686e+01 6.56202572e+01 7.10332117e+01 4.93797984e+01\n",
      "  5.11934269e+01 4.91536849e+01 4.21172514e+01 3.58321896e+01]\n",
      " [4.83510397e-05 5.48072588e-05 6.92200023e-05 3.35715348e-05\n",
      "  7.26165553e-05 8.65084539e-05 9.41101726e-05 6.51617943e-05\n",
      "  6.73264760e-05 6.52249480e-05 5.61023288e-05 4.75213558e-05]\n",
      " [2.81859197e-07 3.21934515e-07 4.05123624e-07 1.94783350e-07\n",
      "  4.25432524e-07 5.04764577e-07 5.50433505e-07 3.81109919e-07\n",
      "  3.90191765e-07 3.82340383e-07 3.27098458e-07 2.78386508e-07]]\n",
      "result_ein: [[4.09144465e+01 4.56754279e+01 5.62733940e+01 2.90068854e+01\n",
      "  5.95864664e+01 7.21382600e+01 7.74379842e+01 5.42451985e+01\n",
      "  5.63564979e+01 5.34810083e+01 4.53639004e+01 3.90167095e+01]\n",
      " [5.26446504e-05 5.83653105e-05 7.47315118e-05 3.70046236e-05\n",
      "  7.80836877e-05 9.40544617e-05 1.01696505e-04 7.03420156e-05\n",
      "  7.46888828e-05 7.00189700e-05 6.13162905e-05 5.11094287e-05]\n",
      " [2.66467313e-07 3.08969657e-07 3.85613307e-07 1.82485207e-07\n",
      "  4.05914462e-07 4.77868436e-07 5.23432320e-07 3.62543991e-07\n",
      "  3.64262606e-07 3.65204409e-07 3.08842824e-07 2.65575310e-07]]\n",
      "\n",
      "Comparison Results\n",
      "Final result matrices close: False\n"
     ]
    }
   ],
   "source": [
    "# Debugging to focus only on the final result\n",
    "def debug_attention_final_result(x, W_O, W_V, W_K, W_Q):\n",
    "    print(\"Debugging Final Result\")\n",
    "    \n",
    "    # ---- Refactored Function ----\n",
    "    A_ref = softmax(np.dot(np.dot(x, W_Q), np.dot(x, W_K).T))\n",
    "    W_V_O_ref = np.dot(W_V, W_O)\n",
    "    result_ref = np.dot(A_ref, np.dot(x, W_V_O_ref))\n",
    "    print(\"result_ref:\", result_ref)\n",
    "    \n",
    "    # ---- Einsum Function ----\n",
    "    Q_ein = np.einsum('nd,df->nf', x, W_Q)\n",
    "    K_ein = np.einsum('nd,df->nf', x, W_K)\n",
    "    A_ein = softmax(np.einsum('nf,mf->nm', Q_ein, K_ein))\n",
    "    W_V_O_ein = np.einsum('df,fg->dg', W_V, W_O)\n",
    "    result_ein = np.einsum('nm,nd,dg->ng', A_ein, x, W_V_O_ein)\n",
    "    print(\"result_ein:\", result_ein)\n",
    "    \n",
    "    # Compare final result\n",
    "    print(\"\\nComparison Results\")\n",
    "    print(\"Final result matrices close:\", np.allclose(result_ref, result_ein))\n",
    "\n",
    "# Debugging with a new set of random weights and input\n",
    "x = np.random.rand(n, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "\n",
    "# Run the debugging function\n",
    "debug_attention_final_result(x, W_O, W_V, W_K, W_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging with Alternative Approaches\n",
      "\n",
      "Results\n",
      "Result using np.matmul in Approach 1: [[1.08048540e-04 1.24148156e-04 1.39292108e-04 1.36772395e-04\n",
      "  1.24422408e-04 1.39042323e-04 1.30770611e-04 1.19650614e-04\n",
      "  1.44268754e-04 1.17531403e-04 1.17849378e-04 1.05803953e-04]\n",
      " [4.90091778e+01 5.67871606e+01 6.47516397e+01 6.28263480e+01\n",
      "  5.71823505e+01 6.38459024e+01 6.06966172e+01 5.47726209e+01\n",
      "  6.60903199e+01 5.37428371e+01 5.39181156e+01 4.83328985e+01]\n",
      " [2.14384554e-16 2.46504306e-16 2.76408024e-16 2.71709242e-16\n",
      "  2.47229796e-16 2.75953639e-16 2.59570572e-16 2.37588654e-16\n",
      "  2.86261541e-16 2.33357345e-16 2.34195273e-16 2.10245049e-16]]\n",
      "Result using np.einsum in Approach 2: [[1.08048540e-04 1.24148156e-04 1.39292108e-04 1.36772395e-04\n",
      "  1.24422408e-04 1.39042323e-04 1.30770611e-04 1.19650614e-04\n",
      "  1.44268754e-04 1.17531403e-04 1.17849378e-04 1.05803953e-04]\n",
      " [4.90091778e+01 5.67871606e+01 6.47516397e+01 6.28263480e+01\n",
      "  5.71823505e+01 6.38459024e+01 6.06966172e+01 5.47726209e+01\n",
      "  6.60903199e+01 5.37428371e+01 5.39181156e+01 4.83328985e+01]\n",
      " [2.14384554e-16 2.46504306e-16 2.76408024e-16 2.71709242e-16\n",
      "  2.47229796e-16 2.75953639e-16 2.59570572e-16 2.37588654e-16\n",
      "  2.86261541e-16 2.33357345e-16 2.34195273e-16 2.10245049e-16]]\n",
      "Result using np.dot in intermediate steps in Approach 3: [[1.08048540e-04 1.24148156e-04 1.39292108e-04 1.36772395e-04\n",
      "  1.24422408e-04 1.39042323e-04 1.30770611e-04 1.19650614e-04\n",
      "  1.44268754e-04 1.17531403e-04 1.17849378e-04 1.05803953e-04]\n",
      " [4.90091778e+01 5.67871606e+01 6.47516397e+01 6.28263480e+01\n",
      "  5.71823505e+01 6.38459024e+01 6.06966172e+01 5.47726209e+01\n",
      "  6.60903199e+01 5.37428371e+01 5.39181156e+01 4.83328985e+01]\n",
      " [2.14384554e-16 2.46504306e-16 2.76408024e-16 2.71709242e-16\n",
      "  2.47229796e-16 2.75953639e-16 2.59570572e-16 2.37588654e-16\n",
      "  2.86261541e-16 2.33357345e-16 2.34195273e-16 2.10245049e-16]]\n",
      "Result using np.einsum in intermediate steps in Approach 4: [[1.08048540e-04 1.24148156e-04 1.39292108e-04 1.36772395e-04\n",
      "  1.24422408e-04 1.39042323e-04 1.30770611e-04 1.19650614e-04\n",
      "  1.44268754e-04 1.17531403e-04 1.17849378e-04 1.05803953e-04]\n",
      " [4.90091778e+01 5.67871606e+01 6.47516397e+01 6.28263480e+01\n",
      "  5.71823505e+01 6.38459024e+01 6.06966172e+01 5.47726209e+01\n",
      "  6.60903199e+01 5.37428371e+01 5.39181156e+01 4.83328985e+01]\n",
      " [2.14384554e-16 2.46504306e-16 2.76408024e-16 2.71709242e-16\n",
      "  2.47229796e-16 2.75953639e-16 2.59570572e-16 2.37588654e-16\n",
      "  2.86261541e-16 2.33357345e-16 2.34195273e-16 2.10245049e-16]]\n",
      "\n",
      "Comparison Results\n",
      "Approaches 1 and 2 close: True\n",
      "Approaches 1 and 3 close: True\n",
      "Approaches 1 and 4 close: True\n",
      "Approaches 2 and 3 close: True\n",
      "Approaches 2 and 4 close: True\n",
      "Approaches 3 and 4 close: True\n"
     ]
    }
   ],
   "source": [
    "# Debugging by trying alternative ways of performing the final multiplication\n",
    "def debug_attention_alternative_approaches(x, W_O, W_V, W_K, W_Q):\n",
    "    print(\"Debugging with Alternative Approaches\")\n",
    "    \n",
    "    # ---- Common Steps ----\n",
    "    # Compute the attention matrix A for both versions\n",
    "    Q = np.dot(x, W_Q)\n",
    "    K = np.dot(x, W_K)\n",
    "    A = softmax(np.dot(Q, K.T))\n",
    "    \n",
    "    # Compute W_V_O transformation matrix for both versions\n",
    "    W_V_O = np.dot(W_V, W_O)\n",
    "    \n",
    "    # ---- Approach 1: Using np.matmul for final multiplication ----\n",
    "    x_transform_1 = np.dot(x, W_V_O)\n",
    "    result_1 = np.matmul(A, x_transform_1)\n",
    "    \n",
    "    # ---- Approach 2: Using np.einsum for final multiplication ----\n",
    "    x_transform_2 = np.einsum('nd,dg->ng', x, W_V_O)\n",
    "    result_2 = np.einsum('nm,mg->ng', A, x_transform_2)\n",
    "    \n",
    "    # ---- Approach 3: Using np.dot for intermediate steps and then np.matmul ----\n",
    "    x_transform_3 = np.dot(x, W_V_O)\n",
    "    result_3 = np.matmul(A, x_transform_3)\n",
    "    \n",
    "    # ---- Approach 4: Using np.einsum for intermediate steps and then np.dot ----\n",
    "    x_transform_4 = np.einsum('nd,dg->ng', x, W_V_O)\n",
    "    result_4 = np.dot(A, x_transform_4)\n",
    "    \n",
    "    print(\"\\nResults\")\n",
    "    print(\"Result using np.matmul in Approach 1:\", result_1)\n",
    "    print(\"Result using np.einsum in Approach 2:\", result_2)\n",
    "    print(\"Result using np.dot in intermediate steps in Approach 3:\", result_3)\n",
    "    print(\"Result using np.einsum in intermediate steps in Approach 4:\", result_4)\n",
    "    \n",
    "    # Compare each approach\n",
    "    print(\"\\nComparison Results\")\n",
    "    print(\"Approaches 1 and 2 close:\", np.allclose(result_1, result_2))\n",
    "    print(\"Approaches 1 and 3 close:\", np.allclose(result_1, result_3))\n",
    "    print(\"Approaches 1 and 4 close:\", np.allclose(result_1, result_4))\n",
    "    print(\"Approaches 2 and 3 close:\", np.allclose(result_2, result_3))\n",
    "    print(\"Approaches 2 and 4 close:\", np.allclose(result_2, result_4))\n",
    "    print(\"Approaches 3 and 4 close:\", np.allclose(result_3, result_4))\n",
    "\n",
    "# Debugging with a new set of random weights and input\n",
    "x = np.random.rand(n, d_model)\n",
    "W_O = np.random.rand(d_model, d_model)\n",
    "W_V = np.random.rand(d_model, d_model)\n",
    "W_K = np.random.rand(d_model, d_model)\n",
    "W_Q = np.random.rand(d_model, d_model)\n",
    "\n",
    "# Run the debugging function\n",
    "debug_attention_alternative_approaches(x, W_O, W_V, W_K, W_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
