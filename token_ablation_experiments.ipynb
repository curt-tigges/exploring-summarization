{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from functools import partial\n",
    "import torch\n",
    "import datasets\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import Dict, Iterable, List, Tuple, Union\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_dataset, tokenize_and_concatenate, get_act_name, test_prompt\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from utils.store import load_array, save_html, save_array, is_file, get_model_name, clean_label, save_text\n",
    "from utils.circuit_analysis import get_logit_diff\n",
    "\n",
    "from utils.tokenwise_ablation import (\n",
    "    compute_ablation_modified_logit_diff,\n",
    "    load_directions,\n",
    "    get_random_directions,\n",
    "    get_zeroed_dir_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma Ablation on Natural Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6731f7f1374d59a3e99ab9ecb579c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5993ee666b164db190cde384ccc6679c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028f7fbd03e9492d80094c708a798f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc98b8f7e7624424a30012ddf06cd8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1635636e173f4420a1fc73439d402c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\"\n",
    "MODEL_NAME = \"EleutherAI/pythia-2.8b\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    device=device,\n",
    ")\n",
    "model.name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory sst2 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/exploring-summarization/token_ablation_experiments.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/exploring-summarization/token_ablation_experiments.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_from_disk\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/exploring-summarization/token_ablation_experiments.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sst_data \u001b[39m=\u001b[39m load_from_disk(\u001b[39m\"\u001b[39;49m\u001b[39msst2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/exploring-summarization/token_ablation_experiments.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sst_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2244\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2241\u001b[0m     path_join \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin\n\u001b[1;32m   2243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fs\u001b[39m.\u001b[39mexists(dest_dataset_path):\n\u001b[0;32m-> 2244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDirectory \u001b[39m\u001b[39m{\u001b[39;00mdataset_path\u001b[39m}\u001b[39;00m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2245\u001b[0m \u001b[39mif\u001b[39;00m fs\u001b[39m.\u001b[39misfile(path_join(dest_dataset_path, config\u001b[39m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[39mand\u001b[39;00m fs\u001b[39m.\u001b[39misfile(\n\u001b[1;32m   2246\u001b[0m     path_join(dest_dataset_path, config\u001b[39m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2247\u001b[0m ):\n\u001b[1;32m   2248\u001b[0m     \u001b[39mreturn\u001b[39;00m Dataset\u001b[39m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory, storage_options\u001b[39m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory sst2 not found"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "sst_data = load_from_disk(\"sst2\")\n",
    "sst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The Rock is destined to be the 21st Century's new ''Conan'' and that he's going to make a splash even greater than Arnold Schwarzenegger, Jean-Claud Van Damme or Steven Segal.\",\n",
       " 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_data['train'][0]['text'], sst_data['train'][0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function filter_function at 0x7f8892bf58b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea70a00a98746abbbe87f3acdbe4717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52919414d384b349cd94e8ef5aeba11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635761b989ca440c9d8a6887d81d9464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5fba363413408986b9d38ef0f71cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c27ca6a45b45a8b1a883229d7ad93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_function(example):\n",
    "    prompt = model.to_tokens(example['text'] + \" Review Sentiment:\", prepend_bos=False)\n",
    "    answer = torch.tensor([29071, 32725]).unsqueeze(0).unsqueeze(0).to(device) if example['label'] == 1 else torch.tensor([32725, 29071]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    logits, cache = model.run_with_cache(prompt)\n",
    "    logit_diff = get_logit_diff(logits, answer)\n",
    "    \n",
    "    # Determine if the top answer (index 0) token is in top 10 logits\n",
    "    _, top_indices = logits.topk(10, dim=-1)  # Get indices of top 10 logits\n",
    "    top_answer_token = answer[0, 0, 0]  # Assuming answer is of shape (1, 1, 2) and the top answer token is at index 0\n",
    "    is_top_answer_in_top_10_logits = (top_indices == top_answer_token).any()\n",
    "    \n",
    "    # Add a new field 'keep_example' to the example\n",
    "    example['keep_example'] = (logit_diff > 0.0) and is_top_answer_in_top_10_logits.item()\n",
    "    return example\n",
    "\n",
    "# Use the map function to apply the filter_function\n",
    "sst_data_with_flag_train = sst_data['train'].map(filter_function, keep_in_memory=True)\n",
    "sst_data_with_flag_val = sst_data['dev'].map(filter_function, keep_in_memory=True)\n",
    "sst_data_with_flag_test = sst_data['test'].map(filter_function, keep_in_memory=True)\n",
    "sst_data_with_flag = concatenate_datasets([sst_data_with_flag_train, sst_data_with_flag_val, sst_data_with_flag_test])\n",
    "\n",
    "# Use the filter function to keep only the examples where 'keep_example' is True\n",
    "sst_zero_shot = sst_data_with_flag.filter(lambda x: x['keep_example'])\n",
    "\n",
    "# save dataset\n",
    "sst_zero_shot.save_to_disk(\"sst_zero_shot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'keep_example'],\n",
       "    num_rows: 6169\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "sst_zero_shot = load_from_disk(\"sst_zero_shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'keep_example', 'tokens', 'attention_mask', 'answers', 'positions', 'has_token'],\n",
       "    num_rows: 3318\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Load a tokenizer (you'll need to specify the appropriate model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "# set padding token\n",
    "tokenizer.pad_token = model.to_string([1])\n",
    "\n",
    "#dataset = text_dataset.map(lambda x: tokenize_and_concatenate(x, tokenizer))\n",
    "\n",
    "def concatenate_classification_prompts(examples):\n",
    "    return {\"text\": (examples['text'] + \" Review Sentiment:\")}\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "\n",
    "def find_dataset_positions(example, token_id=13):\n",
    "    # Create a tensor of zeros with the same shape as example['tokens']\n",
    "    positions = torch.zeros_like(torch.tensor(example['tokens']), dtype=torch.int)\n",
    "\n",
    "    # Find positions where tokens match the given token_id\n",
    "    positions[example['tokens'] == token_id] = 1\n",
    "    has_token = True if positions.sum() > 0 else False\n",
    "\n",
    "    return {'positions': positions, 'has_token': has_token}\n",
    "\n",
    "def convert_answers(example, pos_answer_id=29071, neg_answer_id=32725):\n",
    "    if example['label'] == 1:\n",
    "        answers = torch.tensor([pos_answer_id, neg_answer_id])\n",
    "    else:\n",
    "        answers = torch.tensor([neg_answer_id, pos_answer_id])\n",
    "\n",
    "    return {'answers': answers}\n",
    "\n",
    "\n",
    "dataset = sst_zero_shot.map(concatenate_classification_prompts, batched=False)\n",
    "dataset = dataset.map(tokenize_function, batched=False)\n",
    "dataset = dataset.map(convert_answers, batched=False)\n",
    "dataset = dataset.rename_column(\"input_ids\", \"tokens\")\n",
    "dataset.set_format(type=\"torch\", columns=[\"tokens\", \"attention_mask\", \"label\", \"answers\"])\n",
    "dataset = dataset.map(find_dataset_positions, batched=False)\n",
    "dataset = dataset.filter(lambda example: example['has_token']==True)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The Rock is destined to be the 21st Century's new ''Conan'' and that he's going to make a splash even greater than Arnold Schwarzenegger, Jean-Claud Van Damme or Steven Segal. Review Sentiment:<|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|>\",\n",
       " [' Positive'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(dataset[0]['tokens']), model.to_str_tokens(dataset[0]['answers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.8829, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.circuit_analysis import get_logit_diff\n",
    "logits, cache = model.run_with_cache(dataset['tokens'][0])\n",
    "get_logit_diff(logits, dataset['answers'][0].unsqueeze(0).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2623, 695)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a subset with only positive labels\n",
    "pos_dataset = dataset.filter(lambda example: example['label']==1)\n",
    "neg_dataset = dataset.filter(lambda example: example['label']==0)\n",
    "len(pos_dataset), len(neg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b6c781d72e4c4cb1f37ab0a8e9cb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf96ee56ff442dfbdb212e0cfb09d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd0b2e6b788416381cb7e23381e4dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(139, 139)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def get_random_subset(dataset, n):\n",
    "    total_size = len(dataset)\n",
    "    random_indices = random.sample(range(total_size), n)\n",
    "    return dataset.select(random_indices)\n",
    "\n",
    "pos_subset = get_random_subset(pos_dataset, 695)\n",
    "neg_subset = get_random_subset(neg_dataset, 695)\n",
    "balanced_subset = concatenate_datasets([pos_subset, neg_subset])\n",
    "# randomize the order of balanced_subset\n",
    "balanced_subset = balanced_subset.shuffle(len(balanced_subset))\n",
    "\n",
    "# Create a new dataloader from the subset, converting the data to tensors\n",
    "pos_data_loader = DataLoader(\n",
    "    pos_subset, batch_size=5, shuffle=False, drop_last=True\n",
    ")\n",
    "neg_data_loader = DataLoader(\n",
    "    neg_subset, batch_size=5, shuffle=False, drop_last=True\n",
    ")\n",
    "balanced_data_loader = DataLoader(\n",
    "    balanced_subset, batch_size=5, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# save datasets\n",
    "pos_subset.save_to_disk(\"sst_pos_subset\")\n",
    "neg_subset.save_to_disk(\"sst_neg_subset\")\n",
    "balanced_subset.save_to_disk(\"sst_balanced_subset\")\n",
    "\n",
    "len(pos_data_loader), len(neg_data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OWT Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "owt_data = load_dataset(\"stas/openwebtext-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import OWTData\n",
    "owt_data = OWTData(owt_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d17e88f1a2463292a8f3c0bb8468ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "owt_data.preprocess_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b259922f9924d4f949a648b3285e5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "ExperimentData._find_dataset_positions() got multiple values for argument 'token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/exploring-summarization/token_ablation_experiments.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/exploring-summarization/token_ablation_experiments.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m owt_data\u001b[39m.\u001b[39;49mfind_dataset_positions()\n",
      "File \u001b[0;32m/notebooks/utils/datasets.py:41\u001b[0m, in \u001b[0;36mExperimentData.find_dataset_positions\u001b[0;34m(self, token_to_ablate)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Finds the positions of a given token in the dataset\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_dict\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_dict[split] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_dict[split]\u001b[39m.\u001b[39;49mmap(partial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_dataset_positions, token_id\u001b[39m=\u001b[39;49mtoken_to_ablate))\n\u001b[1;32m     43\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpositions\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_dict[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcolumn_names, \u001b[39m\"\u001b[39m\u001b[39mDataset does not have a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpositions\u001b[39m\u001b[39m'\u001b[39m\u001b[39m column\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "\u001b[0;31mTypeError\u001b[0m: ExperimentData._find_dataset_positions() got multiple values for argument 'token_id'"
     ]
    }
   ],
   "source": [
    "owt_data.find_dataset_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Mean Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2560])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6eebdc064e4f36b2c8e5b547586bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2560])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538bda9679164d36a911f11b55a40932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2560])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ccadcedc84447b829712fcee4d370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'data/pythia-2.8b/comma_pos_mean_values.npy'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_mean_bal_values = get_layerwise_token_mean_activations(model, balanced_data_loader, token_id=13)\n",
    "comma_mean_neg_values = get_layerwise_token_mean_activations(model, neg_data_loader, token_id=13)\n",
    "comma_mean_pos_values = get_layerwise_token_mean_activations(model, pos_data_loader, token_id=13)\n",
    "save_array(comma_mean_bal_values, 'comma_balanced_mean_values.npy', model)\n",
    "save_array(comma_mean_neg_values, 'comma_neg_mean_values.npy', model)\n",
    "save_array(comma_mean_pos_values, 'comma_pos_mean_values.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# period_mean_values = get_layerwise_token_mean_activations(model, train_data_loader, token_id=15)\n",
    "# save_array(period_mean_values, 'period_mean_values.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files\n",
    "owt_mean_values = torch.from_numpy(load_array('comma_mean_values.npy', model)).to(device)\n",
    "comma_mean_bal_values = torch.from_numpy(load_array('comma_balanced_mean_values.npy', model)).to(device)\n",
    "comma_mean_neg_values = torch.from_numpy(load_array('comma_neg_mean_values.npy', model)).to(device)\n",
    "comma_mean_pos_values = torch.from_numpy(load_array('comma_pos_mean_values.npy', model)).to(device)\n",
    "#period_mean_values = torch.from_numpy(load_array('period_mean_values.npy', model)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Loss Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed36f71950b444198cd80f00b1d37d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "layers_to_ablate = [layer for layer in range(model.cfg.n_layers)]\n",
    "ablated_loss_diff, orig_loss = compute_mean_ablation_modified_loss(\n",
    "    model, \n",
    "    balanced_data_loader,\n",
    "    layers_to_ablate,\n",
    "    comma_mean_bal_values,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "ablated_loss = orig_loss + ablated_loss_diff\n",
    "\n",
    "# orig_accuracy = (orig_ld_list > 0).float().mean()\n",
    "# ablated_accuracy = (ablated_ld_list > 0).float().mean()\n",
    "# freeze_ablated_accuracy = (freeze_ablated_ld_list > 0).float().mean()\n",
    "\n",
    "# print(f\"Original mean logit diff: {orig_ld_list.mean():.4f}\")\n",
    "# print(f\"Original accuracy: {orig_accuracy:.4f}\")\n",
    "# print(\"\\n\")\n",
    "# print(f\"Comma-ablated mean logit diff: {ablated_ld_list.mean():.4f}\")\n",
    "# print(f\"Comma-ablated accuracy: {ablated_accuracy:.4f}\")\n",
    "# print(f\"Percent drop in logit diff with comma ablation: {(orig_ld_list.mean() - ablated_ld_list.mean()) / orig_ld_list.mean() * 100:.2f}%\")\n",
    "# print(f\"Percent drop in accuracy with comma ablation: {(orig_accuracy - ablated_accuracy) / orig_accuracy * 100:.2f}%\")\n",
    "# print(\"\\n\")\n",
    "# print(f\"Attn frozen, comma-ablated mean logit diff: {freeze_ablated_ld_list.mean():.4f}\")\n",
    "# print(f\"Attn frozen, comma-ablated accuracy: {freeze_ablated_accuracy:.4f}\")\n",
    "# print(f\"Percent drop in logit diff with attn frozen, comma ablation: {(orig_ld_list.mean() - freeze_ablated_ld_list.mean()) / orig_ld_list.mean() * 100:.2f}%\")\n",
    "# print(f\"Percent drop in accuracy with attn frozen, comma ablation: {(orig_accuracy - freeze_ablated_accuracy) / orig_accuracy * 100:.2f}%\")\n",
    "# print(\"---------------------------------------------------------\")\n",
    "# print(\"Random direction ablation results:\")\n",
    "# print(f\"Comma-ablated mean logit diff: {ablated_ld_list_rand.mean():.4f}\")\n",
    "# print(f\"Percent drop in logit diff with comma ablation: {(orig_ld_list_rand.mean() - ablated_ld_list_rand.mean()) / orig_ld_list_rand.mean() * 100:.2f}%\")\n",
    "# print(f\"Attn frozen, comma-ablated mean logit diff: {freeze_ablated_ld_list_rand.mean():.4f}\")\n",
    "# print(f\"Percent drop in logit diff with attn frozen, comma ablation: {(orig_ld_list_rand.mean() - freeze_ablated_ld_list_rand.mean()) / orig_ld_list_rand.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.1838, -0.0903,  0.0681,  0.1680, -0.0285,  0.2364,  0.4690,  0.3611,\n",
       "         0.2591, -0.3020,  0.3572,  0.0501,  0.2772,  0.1820,  0.0000,  0.4224,\n",
       "         1.6662,  0.6940,  0.1109,  0.0439, -0.2660,  0.2348, -0.6084, -0.7712,\n",
       "        -0.1148, -0.2007,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablated_loss_diff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2007, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablated_loss_diff[0][33], orig_loss[0][33], ablated_loss[0][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(balanced_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(batch['tokens'][0][33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
