{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from functools import partial\n",
    "import torch\n",
    "import datasets\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import Dict, Iterable, List, Tuple, Union\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_dataset, tokenize_and_concatenate, get_act_name, test_prompt\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from utils.store import load_array, save_html, save_array, is_file, get_model_name, clean_label, save_text\n",
    "from utils.circuit_analysis import get_logit_diff\n",
    "\n",
    "from utils.tokenwise_ablation import (\n",
    "    compute_ablation_modified_logit_diff,\n",
    "    load_directions,\n",
    "    get_random_directions,\n",
    "    get_zeroed_dir_vector,\n",
    "    get_layerwise_token_mean_activations\n",
    ")\n",
    "from utils.datasets import OWTData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma Ablation on Natural Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "MODEL_NAME = \"EleutherAI/pythia-2.8b\"\n",
    "TOKEN_ID = 13\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "owt_data = OWTData.from_model(model)\n",
    "owt_data.preprocess_datasets(token_to_ablate=TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = owt_data.dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'attention_mask', 'positions', 'has_token'],\n",
       "    num_rows: 10959\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  249, 46882,    71,   668,   275,  6176,    13,   533,   253,  2208])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]['tokens'][30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', ' Kamp', 'f', '‚Äù', ' in', ' Germany', ',', ' but', ' the', ' government']\n"
     ]
    }
   ],
   "source": [
    "print(model.to_str_tokens(datasets['train'][0]['tokens'][30:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(datasets['train'][0]['positions'][30:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = owt_data.get_dataloaders(batch_size=BATCH_SIZE)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10959, 685)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets['train']), len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9b543ff77c42bfa06498e11602bb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/oskar/projects/exploring-summarization/token_ablation_experiments.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/oskar/projects/exploring-summarization/token_ablation_experiments.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m comma_mean_values \u001b[39m=\u001b[39m get_layerwise_token_mean_activations(model, dataloader, token_id\u001b[39m=\u001b[39;49m\u001b[39m13\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/oskar/projects/exploring-summarization/token_ablation_experiments.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m save_array(comma_mean_values, \u001b[39m'\u001b[39m\u001b[39mcomma_mean_values.npy\u001b[39m\u001b[39m'\u001b[39m, model)\n",
      "File \u001b[0;32m/notebooks/utils/tokenwise_ablation.py:142\u001b[0m, in \u001b[0;36mget_layerwise_token_mean_activations\u001b[0;34m(model, data_loader, token_id, device)\u001b[0m\n\u001b[1;32m    139\u001b[0m token_count: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m _, batch_value \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(data_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_loader)):\n\u001b[0;32m--> 142\u001b[0m     batch_tokens \u001b[39m=\u001b[39m batch_value[\u001b[39m\"\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    144\u001b[0m     \u001b[39m# Get binary mask of positions where token_id matches in the batch of tokens\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     token_mask \u001b[39m=\u001b[39m batch_tokens \u001b[39m==\u001b[39m token_id\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "comma_mean_values = get_layerwise_token_mean_activations(model, dataloader, token_id=13, device=device)\n",
    "save_array(comma_mean_values, 'comma_mean_values.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files\n",
    "owt_mean_values = torch.from_numpy(load_array('comma_mean_values.npy', model)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Loss Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "layers_to_ablate = [layer for layer in range(model.cfg.n_layers)]\n",
    "ablated_loss_diff, orig_loss = compute_mean_ablation_modified_loss(\n",
    "    model, \n",
    "    balanced_data_loader,\n",
    "    layers_to_ablate,\n",
    "    comma_mean_bal_values,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "ablated_loss = orig_loss + ablated_loss_diff\n",
    "\n",
    "# orig_accuracy = (orig_ld_list > 0).float().mean()\n",
    "# ablated_accuracy = (ablated_ld_list > 0).float().mean()\n",
    "# freeze_ablated_accuracy = (freeze_ablated_ld_list > 0).float().mean()\n",
    "\n",
    "# print(f\"Original mean logit diff: {orig_ld_list.mean():.4f}\")\n",
    "# print(f\"Original accuracy: {orig_accuracy:.4f}\")\n",
    "# print(\"\\n\")\n",
    "# print(f\"Comma-ablated mean logit diff: {ablated_ld_list.mean():.4f}\")\n",
    "# print(f\"Comma-ablated accuracy: {ablated_accuracy:.4f}\")\n",
    "# print(f\"Percent drop in logit diff with comma ablation: {(orig_ld_list.mean() - ablated_ld_list.mean()) / orig_ld_list.mean() * 100:.2f}%\")\n",
    "# print(f\"Percent drop in accuracy with comma ablation: {(orig_accuracy - ablated_accuracy) / orig_accuracy * 100:.2f}%\")\n",
    "# print(\"\\n\")\n",
    "# print(f\"Attn frozen, comma-ablated mean logit diff: {freeze_ablated_ld_list.mean():.4f}\")\n",
    "# print(f\"Attn frozen, comma-ablated accuracy: {freeze_ablated_accuracy:.4f}\")\n",
    "# print(f\"Percent drop in logit diff with attn frozen, comma ablation: {(orig_ld_list.mean() - freeze_ablated_ld_list.mean()) / orig_ld_list.mean() * 100:.2f}%\")\n",
    "# print(f\"Percent drop in accuracy with attn frozen, comma ablation: {(orig_accuracy - freeze_ablated_accuracy) / orig_accuracy * 100:.2f}%\")\n",
    "# print(\"---------------------------------------------------------\")\n",
    "# print(\"Random direction ablation results:\")\n",
    "# print(f\"Comma-ablated mean logit diff: {ablated_ld_list_rand.mean():.4f}\")\n",
    "# print(f\"Percent drop in logit diff with comma ablation: {(orig_ld_list_rand.mean() - ablated_ld_list_rand.mean()) / orig_ld_list_rand.mean() * 100:.2f}%\")\n",
    "# print(f\"Attn frozen, comma-ablated mean logit diff: {freeze_ablated_ld_list_rand.mean():.4f}\")\n",
    "# print(f\"Percent drop in logit diff with attn frozen, comma ablation: {(orig_ld_list_rand.mean() - freeze_ablated_ld_list_rand.mean()) / orig_ld_list_rand.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablated_loss_diff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablated_loss_diff[0][33], orig_loss[0][33], ablated_loss[0][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(balanced_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_str_tokens(batch['tokens'][0][33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
